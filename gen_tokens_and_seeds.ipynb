{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59deca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import json\n",
    "from preprocessing_text import read_text, \\\n",
    "                               lower_case, \\\n",
    "                               is_in_par, \\\n",
    "                               remove_point, \\\n",
    "                               get_stopwords, \\\n",
    "                               get_en_dict, \\\n",
    "                               translate_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ff6aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################################################################\n",
    "# Creation of tokens file from the Europarl texts.\n",
    "#####################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d904047",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory of the folder that contains the Europarl text files.\n",
    "path_folder_txt = os.getcwd()+\"/data/texts\"\n",
    "\n",
    "# List where each element represents a sentence (as a str).\n",
    "sentences = read_text(path_folder_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f487a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove sentences enclosed in parentheses.\n",
    "# The reason for this is as follows:\n",
    "# The Europass texts contain sentences with details, which are enclosed \n",
    "# in parentheses, about the developments in line with the European Parliament sitting.\n",
    "# These details are considered not useful for detecting bias.\n",
    "sentences = [sentence for sentence in sentences if not is_in_par(sentence)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f921753e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed seed for generating the same tokens and \n",
    "# seeds of the thesis experiments.\n",
    "seed_value = 42  \n",
    "random.seed(seed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf128e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample of senteces for reducing computational cost.\n",
    "sentences = random.sample(sentences, 900000)\n",
    "# Extraction of tokens from each sentence.\n",
    "sentences_tokens = [sentence.split() for sentence in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d847495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove final point from tokens.\n",
    "sentences_tokens = remove_point(sentences_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e932a66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove final comma (\",\") from tokens.\n",
    "sentences_tokens = [[token.rstrip(',') for token in tokens] for tokens in sentences_tokens]\n",
    "\n",
    "# Replace the specified characters with an empty string.\n",
    "sentences_tokens = [[token.replace('-', '').replace('–', '')\n",
    "                                           .replace(':','')\n",
    "                                           .replace(';','')\n",
    "                                           .replace('\"','')\n",
    "                                           .strip() for token in tokens] for tokens in sentences_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54bc303d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# At this step, there could be some tokens that are equal to '' or \"'\".\n",
    "# There types of tokens are removed.\n",
    "sentences_tokens = [[token for token in tokens if token != '' or token != \"'\"] for tokens in sentences_tokens]\n",
    "\n",
    "# Tokens are converted in lower case.\n",
    "sentences_tokens = lower_case(sentences_tokens)\n",
    "\n",
    "# There could be some equals tokens in a sentence. The duplicates\n",
    "# are removed.\n",
    "sentences_tokens = [list(set(tokens)) for tokens in sentences_tokens]\n",
    "\n",
    "# The empty lists are removed. \n",
    "sentences_tokens = [tokens for tokens in sentences_tokens if tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a353ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting spanish stopwords.\n",
    "stopw = get_stopwords()\n",
    "\n",
    "# Stopwords are removed.\n",
    "sentences_tokens = [[token for token in tokens if token not in stopw] for tokens in sentences_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65612d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving preprocessed sentences tokens in a txt file \n",
    "# that is used for chapter 3 experiments.\n",
    "with open(\"data/tokens/sentences_tokens.txt\", 'w') as file:\n",
    "    for tokens in sentences_tokens:\n",
    "        linea = ','.join(map(str, tokens))  \n",
    "        file.write(linea + '\\n')  \n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3822653f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################################################################\n",
    "# Creation of spanish seeds file from the seeds extracted from https://aclanthology.org/2021.acl-long.148/ and\n",
    "# from https://github.com/PLN-FaMAF/Bias-in-word-embeddings/blob/main/main_tutorial_bias_word_embedding.ipynb\n",
    "#####################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09bb6be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory of the folder that contains the seeds from https://aclanthology.org/2021.acl-long.148/\n",
    "path_seeds = os.getcwd()+\"/data/seeds/gathered_seeds.json\"\n",
    "with open(path_seeds) as f:\n",
    "    json_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36471dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As stated in the thesis (Chapter 3: Analysis of Results), two characteristics, \n",
    "# gender and religion, have been taken into account. For each characteristic, \n",
    "# certain keys (concepts) are employed to detect bias within the D and W spaces.\n",
    "\n",
    "# keys for gender.\n",
    "keys_g = [\"pleasant\", \"unpleasant\", 'instruments', 'weapons','pleasantness','unpleasantness',\n",
    "        'career', 'family', 'math 1', 'arts 1', 'science 1', 'arts 2', \n",
    "        'careers', 'depressed 1', 'physically ill', 'occupations', 'adjectives sensitive',  \n",
    "        'profesiones_neutras', 'verbos', 'profesiones_colectivos', 'sustantivos_abstractos', 'adjetivos_neutros', \n",
    "        'temporary', 'permanent', 'pleasant 6', 'unpleasant 6', 'adjectives appearance','adjectives intelligence', \n",
    "        \"adjectives otherization\", 'adjectives princeton', 'clothing', 'sports', 'family words', 'career words',\n",
    "        'attractive', 'ugliness', 'violence'\n",
    "       ]\n",
    "\n",
    "# keys for religion.\n",
    "keys_r = ['profesiones_neutras', 'verbos', 'profesiones_colectivos', 'sustantivos_abstractos','adjetivos_neutros',\n",
    "          \"pleasant\", 'unpleasant', 'instruments', 'weapons', 'violence', \n",
    "          'attractive', 'ugliness', 'positive_emotion', 'negative_emotion', \n",
    "          'high morality and low\\/neutral warmth','low\\/neutral and morality high warmth',\n",
    "          \"high competence\",'careers', 'depressed 1', 'terrorism', 'sports', 'domestic_work', 'high competence',\n",
    "           'math 1', 'arts 1', 'science 1', 'arts 2', 'christianity', 'islam', 'islam words', 'christianity words'\n",
    "         ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2042a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting english dict using selected keys\n",
    "dict_en = get_en_dict(json_data, keys_g, keys_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f6ffdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The values (seeds) associated to keys are translated, \n",
    "# from english to spanish\n",
    "dict_es = translate_dict(dict_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1f1268",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some preprocessed actions are done \" by hands\". The actions are:\n",
    "#  1)  Some lists are not translated due to query length limit. These are translated by hand. One\n",
    "#      example are the seeds associated to the key 'careers'. These seeds, as others, are translated\n",
    "#      by hands.\n",
    "#  2) Some keys are removed because are associated with gender specific terms and because are not good for \n",
    "#     religious bias. But this is a investigator choice. Every investigator could think to mantain these \n",
    "#     keys. It is recommended to carry out this selection of keys by hand to better check \n",
    "#     which are the best terms to use for measuring the bias. The normal command for removing a key and its\n",
    "#     seeds is:  diact_es.pop(key)\n",
    "#  3) Some seeds are not usefull for detecting bias. These seeds are removed from them key with the\n",
    "#     command: diact_es[key].remove(seed_in_value_list)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6c8e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following items are added to dict_es. These item are extracted from \n",
    "# https://github.com/PLN-FaMAF/Bias-in-word-embeddings/blob/main/main_tutorial_bias_word_embedding.ipynb\n",
    "pn = {'profesiones_neutras' : [\n",
    "    'chofer',\n",
    "    'columnista',\n",
    "    'publicista',\n",
    "    'naturista',\n",
    "    'asistente',\n",
    "    'taxista',\n",
    "    'psiquiatra',\n",
    "    'policía',\n",
    "    'dentista',\n",
    "    'florista',\n",
    "    'docente',\n",
    "    'periodista',\n",
    "    'electricista',\n",
    "    'economista',\n",
    "    'atleta',\n",
    "    'terapeuta',\n",
    "    'piloto',\n",
    "    'modelo',\n",
    "    'estudiante',\n",
    "    'comerciante',\n",
    "    'chef',\n",
    "    'cantante',\n",
    "    'militar'\n",
    "                               ]\n",
    "     }\n",
    "ver = {'verbos' : [ \n",
    "          'comprar',\n",
    "          'vender',\n",
    "          'dormir',\n",
    "          'despertar',\n",
    "          'soñar',\n",
    "          'llorar',\n",
    "          'gritar',\n",
    "          'hablar',\n",
    "          'preguntar',\n",
    "          'pensar',\n",
    "          'inventar',\n",
    "          'bailar',\n",
    "          'cantar',\n",
    "          'cocinar',\n",
    "          'sentir',\n",
    "          'bordar',\n",
    "          'tejer',\n",
    "          'coser',\n",
    "          'razonar',\n",
    "          'argumentar',\n",
    "          'cursar',\n",
    "          'programar'\n",
    "                   ]\n",
    "      }\n",
    "pc = {'profesiones_colectivos' : [\n",
    "'ingeniería',\n",
    "'arquitectura',\n",
    "'psicología',\n",
    "'enfermería',\n",
    "'medicina',\n",
    "'carpintería',\n",
    "'presidencia',\n",
    "'biología',\n",
    "'cocina',\n",
    "'docencia',\n",
    "'abogacía',\n",
    "'cirugía',\n",
    "'neurocirugía',\n",
    "'actuación',\n",
    "'música',\n",
    "'canto'\n",
    "                                   ]\n",
    "     }\n",
    "sa = {'sustantivos_abstractos' : [\n",
    "'inteligencia',\n",
    "'belleza',\n",
    "'humildad',\n",
    "'sabiduría',\n",
    "'poder',\n",
    "'cariño',\n",
    "'bondad',\n",
    "'ambición',\n",
    "'delicadeza',\n",
    "'amabilidad',\n",
    "'paciencia',\n",
    "'popularidad',\n",
    "'fama',\n",
    "'generosidad',\n",
    "'honestidad',\n",
    "'canto',\n",
    "'maldad',\n",
    "'soberbia',\n",
    "'violencia'\n",
    "                                  ]\n",
    "     }\n",
    "an = {'adjetivos_neutros' : [\n",
    "'inteligente',\n",
    "'humilde',\n",
    "'amable',\n",
    "'dulce',\n",
    "'audaz',\n",
    "'paciente',\n",
    "'popular',\n",
    "'flexible',\n",
    "'grande',\n",
    "'brillante',\n",
    "'inocente',\n",
    "'fácil',\n",
    "'agradable',\n",
    "'infeliz',\n",
    "'capaz',\n",
    "'difícil',\n",
    "'temperamental',\n",
    "                            ]\n",
    "     }\n",
    "pfr = {'profesiones_female' : ['arquitecta',\n",
    "                          'ingeniera',\n",
    "                          'diseñadora',\n",
    "                          'doctora',\n",
    "                          'abogada',\n",
    "                          'profesora',\n",
    "                          'contadora',\n",
    "                          'científica',\n",
    "                          'bióloga',\n",
    "                          'cocinera',\n",
    "                          'psicóloga',\n",
    "                          'enfermera',\n",
    "                          'obrera',\n",
    "                          'actriz'\n",
    "                          ]}\n",
    "\n",
    "pmr = {'profesiones_male' : ['arquitecto', 'ingeniero', 'diseñador', 'doctor', 'abogado', 'profesor', \n",
    "                    'contador', 'científico', 'biólogo', 'cocinero', 'psicólogo', 'enfermero', \n",
    "                    'obrero', 'actor']\n",
    "                             }\n",
    "espacio_f = {'espacio_f' : ['mujer', 'ella' , 'chica', 'niña', 'esposa', 'señora', 'hermana', 'madre', 'abuela']}\n",
    "espacio_m = {'espacio_m' : ['hombre', 'él', 'chico', 'niño', 'esposo', 'señor', 'hermano', 'padre','abuelo']}\n",
    "l_d = [pn, ver, pc, sa, an, pfr, pmr, espacio_f, espacio_m]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1491d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updating for dict_es with the items of the above cell\n",
    "for d in l_d :\n",
    "    dict_es.update(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769cd60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dictionary is saved. The file used for the experiment is in data/seeds folder. \n",
    "file = open(\"dict_PMI_WE.json\", \"w\")\n",
    "json.dump(dict_es, file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29470d7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
